"""
Script para consultar documentos usando un LLM para generar respuestas
Combina embeddings para b√∫squeda + LLM para generaci√≥n de respuestas
"""

from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Configuraci√≥n de la base de datos Chroma
CHROMA_HOST = "localhost"
CHROMA_PORT = 8000
COLLECTION_NAME = "documentos_pdf"

# Configuraci√≥n de Ollama
OLLAMA_HOST = "172.16.1.37"
OLLAMA_PORT = 11434
EMBEDDING_MODEL = "nomic-embed-text:latest"  # Para embeddings
LLM_MODEL = "gpt-oss:20b"  # Para generaci√≥n de respuestas

def inicializar_sistema():
    """
    Inicializa el sistema con embeddings y LLM
    """
    try:
        # 1. Embeddings para b√∫squeda sem√°ntica
        embeddings = OllamaEmbeddings(
            model=EMBEDDING_MODEL,
            base_url=f"http://{OLLAMA_HOST}:{OLLAMA_PORT}"
        )
        
        # 2. Base de datos vectorial
        chroma_client = Chroma(
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings,
            persist_directory="chroma_data"
        )
        
        # 3. LLM para generaci√≥n de respuestas
        llm = Ollama(
            model=LLM_MODEL,
            base_url=f"http://{OLLAMA_HOST}:{OLLAMA_PORT}"
        )
        
        # 4. Prompt template para respuestas estructuradas
        prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            Bas√°ndote en el siguiente contexto, responde la pregunta de manera clara y precisa.
            Si la informaci√≥n no est√° en el contexto, ind√≠calo claramente.
            
            Contexto:
            {context}
            
            Pregunta: {question}
            
            Respuesta:"""
        )
        
        # 5. Chain que combina b√∫squeda + generaci√≥n
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=chroma_client.as_retriever(search_kwargs={"k": 5}),
            chain_type_kwargs={"prompt": prompt_template}
        )
        
        return qa_chain, chroma_client
        
    except Exception as e:
        print(f"Error al inicializar el sistema: {e}")
        return None, None

def consultar_con_llm(qa_chain, query):
    """
    Realiza una consulta usando el LLM para generar respuestas
    """
    try:
        print(f"\nü§ñ Generando respuesta para: '{query}'")
        print("=" * 60)
        
        # Obtener respuesta del LLM
        response = qa_chain.invoke({"query": query})
        
        print(f"üìù Respuesta generada:")
        print(response["result"])
        
        return response["result"]
        
    except Exception as e:
        print(f"Error al consultar: {e}")
        return None

def obtener_estadisticas(chroma_client):
    """
    Obtiene estad√≠sticas de la base de datos
    """
    try:
        collection = chroma_client._collection
        count = collection.count()
        
        print(f"\nüìä Estad√≠sticas del sistema:")
        print(f"‚Ä¢ Documentos en BD: {count}")
        print(f"‚Ä¢ Modelo de embeddings: {EMBEDDING_MODEL}")
        print(f"‚Ä¢ Modelo LLM: {LLM_MODEL}")
        print(f"‚Ä¢ Servidor: {OLLAMA_HOST}:{OLLAMA_PORT}")
        
        return count
        
    except Exception as e:
        print(f"Error al obtener estad√≠sticas: {e}")
        return 0

def main():
    """
    Funci√≥n principal
    """
    print("=== Sistema de Consulta con LLM ===")
    print("Combina embeddings para b√∫squeda + LLM para respuestas\n")
    
    # Inicializar sistema
    qa_chain, chroma_client = inicializar_sistema()
    
    if not qa_chain:
        print("‚ùå Error al inicializar el sistema")
        return
    
    print("‚úÖ Sistema inicializado correctamente")
    
    # Obtener estad√≠sticas
    obtener_estadisticas(chroma_client)
    
    # Ejemplos de consultas
    consultas_ejemplo = [
        "¬øQu√© es YOLOv11 y cu√°les son sus caracter√≠sticas principales?",
        "¬øCu√°les son las mejoras de YOLOv11 sobre versiones anteriores?",
        "¬øQu√© tareas de visi√≥n por computadora soporta YOLOv11?",
        "¬øCu√°l es la diferencia entre YOLOv10 y YOLOv11?"
    ]
    
    print(f"\nüí° Consultas de ejemplo:")
    for i, query in enumerate(consultas_ejemplo, 1):
        print(f"{i}. {query}")
    
    # Realizar consultas
    while True:
        print(f"\n" + "="*60)
        query = input("ü§î Ingresa tu consulta (o 'salir' para terminar): ").strip()
        
        if query.lower() in ['salir', 'exit', 'quit']:
            break
        
        if query:
            consultar_con_llm(qa_chain, query)
        else:
            print("Por favor ingresa una consulta v√°lida")

if __name__ == "__main__":
    main()
