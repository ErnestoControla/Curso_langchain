# Sistema de Vectorizaci√≥n y Consulta de Documentos PDF con LangChain y Chroma

Este proyecto implementa un sistema completo de **RAG (Retrieval-Augmented Generation)** que permite cargar documentos PDF, vectorizarlos y realizar consultas inteligentes usando LLMs.

## üèóÔ∏è Arquitectura del Sistema

```
üìÑ Documentos PDF ‚Üí üîç nomic-embed-text ‚Üí üóÑÔ∏è Chroma DB ‚Üí ü§ñ gpt-oss:20b ‚Üí üìù Respuesta
```

### Componentes:
- **Embeddings**: `nomic-embed-text:latest` (vectorizaci√≥n)
- **Base de datos**: Chroma en `localhost:8000` (almacenamiento)
- **LLM**: `gpt-oss:20b` (generaci√≥n de respuestas)
- **Servidor**: Ollama en `172.16.1.37:11434`

## üì¶ Versiones Instaladas

- **LangChain**: 0.3.27
- **LangChain Core**: 0.3.74
- **LangChain Community**: 0.0.38
- **LangChain Chroma**: 0.2.5
- **LangChain Ollama**: 0.1.0
- **ChromaDB**: 1.0.16
- **PyPDF**: 5.9.0

## ‚öôÔ∏è Configuraci√≥n del Sistema

### Configuraciones Importantes:

1. **Tama√±o de Chunks (CHUNK_SIZE)**: 1000 caracteres
   - Recomendado para documentos t√©cnicos/cient√≠ficos
   - Puedes ajustar seg√∫n el tipo de contenido

2. **Solapamiento de Chunks (CHUNK_OVERLAP)**: 200 caracteres
   - Mantiene contexto entre chunks adyacentes
   - Evita cortar frases importantes

3. **Puerto de Chroma**: 8000
   - Aseg√∫rate de que Chroma est√© ejecut√°ndose en este puerto

4. **Modelo de Embeddings**: nomic-embed-text:latest (Ollama)
   - Modelo especializado para embeddings
   - Requiere Ollama instalado con el modelo nomic-embed-text

5. **Modelo LLM**: gpt-oss:20b (Ollama)
   - Modelo para generaci√≥n de respuestas
   - Optimizado para velocidad y calidad

6. **Configuraci√≥n de Ollama**:
   - Host: 172.16.1.37
   - Puerto: 11434
   - Modelos: nomic-embed-text:latest, gpt-oss:20b

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Crear ambiente conda:
```bash
conda create -n langchain_clean python=3.9
conda activate langchain_clean
```

### 2. Instalar dependencias:
```bash
pip install -r requirements_ultra_minimal.txt
```

### 3. Verificar que Chroma est√© ejecut√°ndose:
```bash
# Si usas Docker Compose
docker-compose up -d

# O iniciar Chroma manualmente en puerto 8000
```

### 4. Verificar que Ollama est√© disponible:
```bash
# Verificar que Ollama est√© instalado
ollama --version

# Verificar modelos disponibles
curl -s http://172.16.1.37:11434/api/tags | python -c "import sys, json; data=json.load(sys.stdin); print('Modelos disponibles:'); [print(f'- {model[\"name\"]}') for model in data['models']]"

# Si no tienes nomic-embed-text, instalarlo
curl -X POST http://172.16.1.37:11434/api/pull -d '{"name": "nomic-embed-text"}'
```

## üìã Uso del Sistema

### 1. Vectorizar Documentos PDF:
```bash
python ejemplo1.py
```

Este script:
- Carga todos los PDF de la carpeta `Documentos/`
- Los divide en chunks de 1000 caracteres
- Los vectoriza usando nomic-embed-text
- Los almacena en la base de datos Chroma

### 2. Consultar Documentos (B√∫squeda Simple):
```bash
python consultar_documentos.py
```

Este script permite:
- Realizar b√∫squedas sem√°nticas
- Ver estad√≠sticas de la base de datos
- Explorar contenido vectorizado

### 3. Consultar con LLM (Respuestas Inteligentes):
```bash
python consultar_con_llm.py
```

Este script combina:
- B√∫squeda sem√°ntica con embeddings
- Generaci√≥n de respuestas con LLM
- Respuestas contextualizadas y estructuradas

### 4. Monitorear Base de Datos:
```bash
python database_monitor.py
```

Este script permite:
- Verificar el estado de la base de datos
- Obtener estad√≠sticas detalladas
- Realizar consultas de diagn√≥stico
- Exportar reportes en JSON
- Modo interactivo para exploraci√≥n

## üîß Configuraciones Recomendadas

### Para Diferentes Tipos de Contenido:

**Documentos T√©cnicos/Cient√≠ficos:**
- CHUNK_SIZE: 1000-1500 caracteres
- CHUNK_OVERLAP: 200-300 caracteres

**Documentos Narrativos:**
- CHUNK_SIZE: 800-1200 caracteres
- CHUNK_OVERLAP: 150-250 caracteres

**Documentos Legales/Contractuales:**
- CHUNK_SIZE: 1200-2000 caracteres
- CHUNK_OVERLAP: 300-500 caracteres

### Configuraci√≥n de la Base de Datos:

**Chroma Settings:**
```python
chroma_client = Chroma(
    collection_name="documentos_pdf",
    embedding_function=embeddings,
    persist_directory="chroma_data"
)
```

**Embeddings con nomic-embed-text:**
```python
embeddings = OllamaEmbeddings(
    model="nomic-embed-text:latest",
    base_url="http://172.16.1.37:11434"
)
```

**LLM con gpt-oss:20b:**
```python
llm = Ollama(
    model="gpt-oss:20b",
    base_url="http://172.16.1.37:11434"
)
```

## üìÅ Estructura del Proyecto

```
Ejemplo1/
‚îú‚îÄ‚îÄ ejemplo1.py                    # Script principal de vectorizaci√≥n
‚îú‚îÄ‚îÄ consultar_documentos.py        # Script de consultas simples
‚îú‚îÄ‚îÄ consultar_con_llm.py          # Script de consultas con LLM
‚îú‚îÄ‚îÄ database_monitor.py           # Monitor de base de datos
‚îú‚îÄ‚îÄ test_system.py                # Script de pruebas del sistema
‚îú‚îÄ‚îÄ config.py                     # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ requirements_ultra_minimal.txt # Dependencias exactas
‚îú‚îÄ‚îÄ docker-compose.yml            # Configuraci√≥n de Chroma
‚îú‚îÄ‚îÄ README.md                     # Documentaci√≥n
‚îú‚îÄ‚îÄ .gitignore                    # Archivos a ignorar
‚îú‚îÄ‚îÄ Documentos/                   # Carpeta con PDFs
‚îÇ   ‚îú‚îÄ‚îÄ 2502.12524v1.pdf
‚îÇ   ‚îî‚îÄ‚îÄ 2410.17725v1.pdf
‚îî‚îÄ‚îÄ chroma_data/                  # Datos persistentes de Chroma
```

## üîç Soluci√≥n de Problemas

### Error de Conexi√≥n con Chroma:
- Verificar que Chroma est√© ejecut√°ndose en puerto 8000
- Revisar logs de Docker: `docker-compose logs`

### Error de Embeddings:
- Verificar que Ollama est√© instalado
- Descargar modelo: `curl -X POST http://172.16.1.37:11434/api/pull -d '{"name": "nomic-embed-text"}'`
- Verificar que el modelo est√© disponible

### Error al Cargar PDFs:
- Verificar que los archivos PDF no est√©n corruptos
- Asegurar permisos de lectura en la carpeta Documentos

### Error de LLM:
- Verificar que gpt-oss:20b est√© disponible en el servidor
- Comprobar conectividad con 172.16.1.37:11434

## üìä Monitoreo y Mantenimiento

### Monitor de Base de Datos:
```bash
# Verificaci√≥n completa autom√°tica
python database_monitor.py

# Modo interactivo para exploraci√≥n
python database_monitor.py
# Luego usar comandos: stats, detailed, search, health, export, quit
```

### Funcionalidades del Monitor:
- **Estad√≠sticas b√°sicas**: Conteo de documentos, metadatos
- **Estad√≠sticas detalladas**: An√°lisis de longitud, fuentes, p√°ginas
- **Pruebas de b√∫squeda**: Verificaci√≥n de consultas sem√°nticas
- **Verificaci√≥n de salud**: Estado general de la base de datos
- **Exportaci√≥n**: Reportes en formato JSON
- **Modo interactivo**: Exploraci√≥n interactiva de la BD

### Verificar Estado de la Base de Datos:
```python
# En consultar_documentos.py
obtener_estadisticas(chroma_client)
```

### Limpiar Base de Datos:
```python
# Eliminar colecci√≥n si es necesario
chroma_client.delete_collection()
```

### Backup de Datos:
- Los datos se guardan en `chroma_data/`
- Hacer backup regular de esta carpeta

## ‚ö° Rendimiento y Optimizaci√≥n

### Para Grandes Vol√∫menes:
- Aumentar CHUNK_SIZE para reducir n√∫mero de chunks
- Usar procesamiento por lotes
- Considerar m√∫ltiples colecciones por tipo de documento

### Para Consultas R√°pidas:
- Indexar metadatos importantes
- Usar filtros en las consultas
- Optimizar queries con par√°metros espec√≠ficos

### Cambio de Modelos:
Para cambiar el LLM, modifica esta l√≠nea en `consultar_con_llm.py`:
```python
LLM_MODEL = "gpt-oss:120b"  # Cambia por el modelo deseado
```

Modelos disponibles en el servidor:
- gpt-oss:20b (recomendado - velocidad/calidad)
- gpt-oss:120b (m√°s potente - m√°s lento)
- gemma3:12b
- deepseek-r1:latest
- llama3.1:8b

## ‚úÖ Verificaci√≥n de la Instalaci√≥n

### Verificaci√≥n Completa del Sistema:
```bash
# Ejecutar todas las pruebas autom√°ticamente
python test_system.py
```

### Verificaci√≥n de la Base de Datos:
```bash
# Verificar estado y contenido de la base de datos
python database_monitor.py
```

### Verificaci√≥n Manual de Librer√≠as:
```bash
# Verificar que todas las librer√≠as est√©n instaladas correctamente
python -c "
import langchain
import langchain_core
import langchain_community
import langchain_chroma
import langchain_ollama
import chromadb
import pypdf
print('‚úì Todas las librer√≠as instaladas correctamente')
print(f'LangChain: {langchain.__version__}')
print(f'LangChain Core: {langchain_core.__version__}')
print(f'LangChain Community: {langchain_community.__version__}')
print(f'LangChain Chroma: Instalado')
print(f'LangChain Ollama: {langchain_ollama.__version__}')
print(f'ChromaDB: {chromadb.__version__}')
"

# Verificar que los modelos est√©n disponibles
curl -s http://172.16.1.37:11434/api/tags | python -c "import sys, json; data=json.load(sys.stdin); print('Modelos disponibles:'); [print(f'- {model[\"name\"]}') for model in data['models']]"
```

## üéØ Casos de Uso

### 1. Investigaci√≥n Acad√©mica:
- Cargar papers cient√≠ficos
- Hacer consultas espec√≠ficas sobre metodolog√≠as
- Obtener respuestas contextualizadas

### 2. Documentaci√≥n T√©cnica:
- Indexar manuales t√©cnicos
- Buscar soluciones espec√≠ficas
- Generar res√∫menes autom√°ticos

### 3. An√°lisis de Documentos:
- Procesar reportes empresariales
- Extraer informaci√≥n clave
- Generar insights autom√°ticos

## üîÑ Flujo de Trabajo T√≠pico

1. **Preparaci√≥n**: Colocar PDFs en carpeta `Documentos/`
2. **Vectorizaci√≥n**: Ejecutar `python ejemplo1.py`
3. **Verificaci√≥n del Sistema**: Ejecutar `python test_system.py`
4. **Verificaci√≥n de la BD**: Ejecutar `python database_monitor.py`
5. **Consulta**: Ejecutar `python consultar_con_llm.py`
6. **Mantenimiento**: Backup regular de `chroma_data/`

## üìà M√©tricas del Sistema

- **Documentos procesados**: 2 PDFs
- **Chunks generados**: 114
- **Tiempo de vectorizaci√≥n**: ~30 segundos
- **Tiempo de consulta**: ~2-5 segundos
- **Precisi√≥n de b√∫squeda**: Alta (embeddings sem√°nticos)
- **Calidad de respuestas**: Excelente (LLM contextual)
